{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vehicle Price Predictor \n",
    "    \n",
    "    In this project I'm going to build a multiple regression model that can predict cars price based on multiple features such as mileage, make, and year. The data we will work with was extracted from a famous data science platform called \"https://www.kaggle.com/jpayne/852k-used-car-listings\"Kaggle. The project will be presented as follow: \n",
    "   \n",
    "        Import Dependencies\n",
    "        Data Preprocessing & Cleansing\n",
    "        Exploratory data analysis & Visualisation\n",
    "        Data Modeling\n",
    "        Evaluting the Model\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data colelction and preprocessing\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import csv\n",
    "# for data visualisation and statistical analysis\n",
    "import numpy as np\n",
    "# from sklearn.cross_validation import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "sns.set_style(\"white\")\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import rcParams\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the column names\n",
    "# Price\tYear\n",
    "#Mileage\n",
    "#City\n",
    "#State\n",
    "#Vin\n",
    "#Make\n",
    "#Model\n",
    "\n",
    "colnames=['Price','Mileage','City','State','Vin','Make','Model'] \n",
    "# read the csv file as a dataframe\n",
    "df = pd.read_csv(\"VehicleTrader/Data/true_car_listings.csv\", sep=\",\",header='infer',low_memory=False)\n",
    "# let's get some simple vision on our dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Price Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change it to integer value\n",
    "df.Price = pd.to_numeric(df.Price, errors = 'coerce', downcast= 'integer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NJ Cars Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.State.str.contains(\"NJ\") == True]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Year Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Year = pd.to_numeric(df.Year, errors = 'coerce', downcast = 'integer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mileage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make  `mileage` numeric "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Mileage = pd.to_numeric(df.Mileage, errors = 'coerce', downcast = 'integer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove spaces from the Make\n",
    "df.Make = df.Make.map(lambda x: x.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove spaces from Model\n",
    "df.Model = df.Model.map(lambda x: x.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop unwanted columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['Vin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory data analysis & Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### price distribution by `Year`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll visualize the distribution of cars price by their year model release, and see how it acts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we set the figure size to 15x8\n",
    "plt.figure(figsize=(15, 8))\n",
    "# plot two values price per year_model\n",
    "plt.scatter(df.Price, df.Year)\n",
    "plt.xlabel(\"price ($)\", fontsize=14)\n",
    "plt.ylabel(\"year of model\", fontsize=14)\n",
    "plt.title(\"Scatter plot of price and year of model\",fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the plot above, the cars price increase respectivly by years, and more explicitly we can say that the more the car is recently released, the price augment, while in the other side the oldest cars still have a low price, and this is totally logical since whenever the cars become kind of old from the date of release, so their price start decrease."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Price distribution by `Make`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we're looking to express the cars price by different features, so one of the important plot is to visualize how these prices differs between cars marks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(15, 12))\n",
    "sns.stripplot(data = df, x='Price', y='Make', jitter=.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plot above, we can extract some the following insights\n",
    "The popular makes such as Honda, Ford, Nissan and Toyota had a stable range of price, in other words they are not well diversified on the price axis.\n",
    "    In the opposit side, we can clearly notice that the sophisticated cars are well distributed over the price axis such as the Mercedes-Benz, Lamborgini, Ferrari, Maserati, Bentley ..., which means that the more the cars from those classes, the more their price augments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 20 Make Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Make feature we have 53 make, so plotting it all is not a good option for visual purpose, we will plot only the top 20 makes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The length of unique make feature is',len(df.Make.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(17,8))\n",
    "df.Make.value_counts().nlargest(20).plot(kind='barh')\n",
    "plt.xlabel('Marks Frequency')\n",
    "plt.title(\"Frequency of TOP 20 Makes Distribution\",fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Price Distribution by Make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(15, 10))\n",
    "sns.stripplot(data = df, x='Year', y='Price', jitter=.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Insights with Violin plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This chart is a combination of a Box Plot and a Density Plot that is rotated and placed on each side, to show the distribution shape of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "MostPopular = df.groupby(['Make']).size()\n",
    "# MostPopular.sort_values(by='count', ascending=False)\n",
    "MostPopular.sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Makes = ['Honda', 'Ford', 'Nissan', 'Toyota', 'BMW']\n",
    "\n",
    "Top5MostPopular = df[df.Make.isin(Makes)]\n",
    "Top5MostPopular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "f, ax = plt.subplots(figsize=(15, 10))\n",
    "\n",
    "sns.violinplot(data = Top5MostPopular, x='Make', y='Price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plot above, we can clearly visualise a lot of information such as the minimum, maximum price for 'most popular' cars and also get perception on the Median values, but more particularly what we got in violin plot other than the box plot, is the density plot width known as Kernel Density Plots. \"The Japenese cars have a less range/spread. Whereas, BMW has largest range.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Price distribution by mileage and fuel type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following plot we will visualize the price distribution by the mileage values groupping by the fuel type and draw the best fit line that express the price (target feature) by mileage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work in Progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although weak, it appears that there seems to be a positive relationship. Let's see what is the actual correlation between price and the other data points. We will look at this in 2 ways heatman for visualization and the correlation coefficient score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(15, 10))\n",
    "# Compute the correlation matrix\n",
    "corr = df.corr()\n",
    "#print(corr)\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.zeros_like(corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
    "plt.title('Correlation matrix', \n",
    "          fontsize = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribution by city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(17,8))\n",
    "df.City.value_counts().nlargest(20).plot(kind='bar')\n",
    "plt.xlabel('City Frequency')\n",
    "plt.title(\"Frequency of TOP 20 Cities\",fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly visualize that most of ad publications are coming from Burlington & Jersey City, which is quite normal due to the geographic distribution of the population in those cities, furthermore the economic position of those cities beyond the other ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Modeling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the moment we will use the K nearset neighbors regressor model with only numerical features, to get a basic view on our model how it behaves, then we will add other categorical features to improve it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a dataframefor testing\n",
    "data = df[df.Price < 100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(data))\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with Categorical Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the moment we still have 3 categorical features which are the year, mark and model the aim of this section is to pre process those features in order to make them numerical so that they will fit into our model.\n",
    "However, we will try to add to fuel_type. \n",
    " \n",
    "In litterature there is two famous kind of categorical variable transformation, the first one is label encoding, and the second one is the one hot encoding, for this use case we will use the one hot position  and the reason why I will choose this kind of data labeling is because I will not need any kind of data normalisation later, and also This has the benefit of not weighting a value improperly but does have the downside of adding more columns to the data set. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[['Year', 'Mileage', 'Make']]\n",
    "Y = data.Price\n",
    "X = pd.get_dummies(data=X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Splitting "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually we split our data into three parts : Training , validation and Testing set, but for simplicity we will use only train and test with 20% in test size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we use the train_test_split function already available in sklearn library to split our data set\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = .20, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import neighbors\n",
    "# the value of n_neighbors will be changed when we plot the histogram showing the lowest RMSE value\n",
    "knn = neighbors.KNeighborsRegressor(n_neighbors=6)\n",
    "knn.fit(X_train, Y_train)\n",
    "\n",
    "predicted = knn.predict(X_test)\n",
    "residual = Y_test - predicted\n",
    "\n",
    "fig = plt.figure(figsize=(30,30))\n",
    "ax1 = plt.subplot(211)\n",
    "sns.distplot(residual, color ='teal')\n",
    "plt.tick_params(axis='both', which='major', labelsize=20)\n",
    "plt.title('Residual counts',fontsize=35)\n",
    "plt.xlabel('Residual',fontsize=25)\n",
    "plt.ylabel('Count',fontsize=25)\n",
    "\n",
    "ax2 = plt.subplot(212)\n",
    "plt.scatter(predicted, residual, color ='teal')\n",
    "plt.tick_params(axis='both', which='major', labelsize=20)\n",
    "plt.xlabel('Predicted',fontsize=25)\n",
    "plt.ylabel('Residual',fontsize=25)\n",
    "plt.axhline(y=0)\n",
    "plt.title('Residual vs. Predicted',fontsize=35)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "rmse = np.sqrt(mean_squared_error(Y_test, predicted))\n",
    "print('RMSE:')\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "print('Variance score: %.2f' % r2_score(Y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as we can see we got 34% in the r^2 score by using n_neighbors = 6, we still don't know if it's the optimal number of neighors or not, so for that we will plot a histogram of different Root Mean Squared Error by n_neighbors and see who's have the lowest RMSE value, and another thing is that the mean of cross validation values is very low which may indicate that our model had overfitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_l = []\n",
    "num = []\n",
    "for n in range(2, 16):\n",
    "    knn = neighbors.KNeighborsRegressor(n_neighbors=n)\n",
    "    knn.fit(X_train, Y_train)\n",
    "    predicted = knn.predict(X_test)\n",
    "    rmse_l.append(np.sqrt(mean_squared_error(Y_test, predicted)))\n",
    "    num.append(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plt = pd.DataFrame()\n",
    "df_plt['rmse'] = rmse_l\n",
    "df_plt['n_neighbors'] = num\n",
    "ax = plt.figure(figsize=(15,7))\n",
    "sns.barplot(data = df_plt, x = 'n_neighbors', y = 'rmse')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that 6 nearest neighbors is the optimal number of neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descision Tree Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "dtr = DecisionTreeRegressor(max_features='auto')\n",
    "dtr.fit(X_train, Y_train)\n",
    "predicted = dtr.predict(X_test)\n",
    "residual = Y_test - predicted\n",
    "\n",
    "fig = plt.figure(figsize=(30,30))\n",
    "ax1 = plt.subplot(211)\n",
    "sns.distplot(residual, color ='orange')\n",
    "plt.tick_params(axis='both', which='major', labelsize=20)\n",
    "plt.title('Residual counts',fontsize=35)\n",
    "plt.xlabel('Residual',fontsize=25)\n",
    "plt.ylabel('Count',fontsize=25)\n",
    "\n",
    "ax2 = plt.subplot(212)\n",
    "plt.scatter(predicted, residual, color ='orange')\n",
    "plt.tick_params(axis='both', which='major', labelsize=20)\n",
    "plt.xlabel('Predicted',fontsize=25)\n",
    "plt.ylabel('Residual',fontsize=25)\n",
    "plt.axhline(y=0)\n",
    "plt.title('Residual vs. Predicted',fontsize=35)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "rmse = np.sqrt(mean_squared_error(Y_test, predicted))\n",
    "print('RMSE:')\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Variance score: %.2f' % r2_score(Y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The root-mean-square deviation (RMSD) or root-mean-square error (RMSE) (or sometimes root-mean-squared error) is a frequently used measure of the differences between values (sample and population values) predicted by a model or an estimator and the values actually observed. The RMSD represents the sample standard deviation of the differences between predicted values and observed values. These individual differences are called residuals when the calculations are performed over the data sample that was used for estimation, and are called prediction errors when computed out-of-sample. The RMSD serves to aggregate the magnitudes of the errors in predictions for various times into a single measure of predictive power. RMSD is a measure of accuracy, to compare forecasting errors of different models for a particular data and not between datasets, as it is scale-dependent. ~ WikiPedia\n",
    "By comparing the Tree Regression with the KNN Regression we can see that the RMSE was reduced from 37709 to 34392 which let us say that this model is more accurate than the last one, but that's not all of it, we still have to test other regression algorithm to check if there is any improvement in result.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at the last RMSE score we've vast improvements, as you can see from the \"Residual vs. Predicted\" that the predicted score is closer to zero and is tighter around the lines which means that we are guessing alot closer to the price."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(X_train, Y_train)\n",
    "\n",
    "predicted = regr.predict(X_test)\n",
    "residual = Y_test - predicted\n",
    "\n",
    "fig = plt.figure(figsize=(30,30))\n",
    "ax1 = plt.subplot(211)\n",
    "sns.distplot(residual, color ='teal')\n",
    "plt.tick_params(axis='both', which='major', labelsize=20)\n",
    "plt.title('Residual counts',fontsize=35)\n",
    "plt.xlabel('Residual',fontsize=25)\n",
    "plt.ylabel('Count',fontsize=25)\n",
    "\n",
    "ax2 = plt.subplot(212)\n",
    "plt.scatter(predicted, residual, color ='teal')\n",
    "plt.tick_params(axis='both', which='major', labelsize=20)\n",
    "plt.xlabel('Predicted',fontsize=25)\n",
    "plt.ylabel('Residual',fontsize=25)\n",
    "plt.axhline(y=0)\n",
    "plt.title('Residual vs. Predicted',fontsize=35)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "rmse = np.sqrt(mean_squared_error(Y_test, predicted))\n",
    "print('RMSE:')\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Variance score: %.2f' % r2_score(Y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boosting is a machine learning ensemble meta-algorithm for primarily reducing bias, and also variance in supervised learning, and a family of machine learning algorithms which convert weak learners to strong ones. Boosting is based on the question posed by Kearns and Valiant (1988, 1989): Can a set of weak learners create a single strong learner? A weak learner is defined to be a classifier which is only slightly correlated with the true classification (it can label examples better than random guessing). In contrast, a strong learner is a classifier that is arbitrarily well-correlated with the true classification. ~ WikiPedia)\n",
    "Let's see if boosting can improve our scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "r_sq = []\n",
    "deep = []\n",
    "mean_scores = []\n",
    "\n",
    "#loss : {‘ls’, ‘lad’, ‘huber’, ‘quantile’}\n",
    "for n in range(3, 11):\n",
    "    gbr = GradientBoostingRegressor(loss ='ls', max_depth=n)\n",
    "    gbr.fit (X, Y)\n",
    "    deep.append(n)\n",
    "    r_sq.append(gbr.score(X, Y))\n",
    "    mean_scores.append(cross_val_score(gbr, X, Y, cv=6).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt_gbr = pd.DataFrame()\n",
    "\n",
    "plt_gbr['mean_scores'] = mean_scores\n",
    "plt_gbr['depth'] = deep\n",
    "plt_gbr['R²'] = r_sq\n",
    "\n",
    "f, ax = plt.subplots(figsize=(15, 5))\n",
    "sns.barplot(data = plt_gbr, x='depth', y='R²')\n",
    "plt.show()\n",
    "\n",
    "f, ax = plt.subplots(figsize=(15, 5))\n",
    "sns.barplot(data = plt_gbr, x='depth', y='mean_scores')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "gbr = GradientBoostingRegressor(loss ='ls', max_depth=6)\n",
    "gbr.fit (X_train, Y_train)\n",
    "predicted = gbr.predict(X_test)\n",
    "residual = Y_test - predicted\n",
    "\n",
    "fig = plt.figure(figsize=(30,30))\n",
    "ax1 = plt.subplot(211)\n",
    "sns.distplot(residual, color ='teal')\n",
    "plt.tick_params(axis='both', which='major', labelsize=20)\n",
    "plt.title('Residual counts',fontsize=35)\n",
    "plt.xlabel('Residual',fontsize=25)\n",
    "plt.ylabel('Count',fontsize=25)\n",
    "\n",
    "ax2 = plt.subplot(212)\n",
    "plt.scatter(predicted, residual, color ='teal')\n",
    "plt.tick_params(axis='both', which='major', labelsize=20)\n",
    "plt.xlabel('Predicted',fontsize=25)\n",
    "plt.ylabel('Residual',fontsize=25)\n",
    "plt.axhline(y=0)\n",
    "plt.title('Residual vs. Predicted',fontsize=35)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(Y_test, predicted))\n",
    "scores = cross_val_score(gbr, X, Y, cv=12)\n",
    "\n",
    "print('\\nCross Validation Scores:')\n",
    "print(scores)\n",
    "print('\\nMean Score:')\n",
    "print(scores.mean())\n",
    "print('\\nRMSE:')\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Variance score: %.2f' % r2_score(Y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction VS Real price histogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all we reshape our data to a 1D array then we plot the histogram doing the comparison between the real price and the predicted ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = Y_test.reshape(-1, 1)\n",
    "B = predicted.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = 16,5\n",
    "plt.figure()\n",
    "plt.plot(A[-100:], label=\"Real\")\n",
    "plt.plot(B[-100:], label=\"Predicted\")\n",
    "plt.legend()\n",
    "plt.title(\"Price: real vs predicted\")\n",
    "plt.ylabel(\"Price [$]\")\n",
    "plt.xticks(())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can notice clearly that the two line (real vs predicted) fit each other well, with some small differences which let us say that we did a good improvement compared with the first model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that the Gradient Boosting model regressor win the battle with the lowest RMSE value and the highest R^2 score. In the following table we will do a benchmarking resuming all the models tested above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table class=\"table table-bordered\">\n",
    "    <thead>\n",
    "      <tr>\n",
    "        <th>Model</th>\n",
    "        <th>Variance Score</th>\n",
    "        <th>RMSE</th>\n",
    "      </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "      <tr>\n",
    "        <td>KNN</td>\n",
    "        <td>0.56</td>\n",
    "        <td>37709.67</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td>Multiple Linear Regression</td>\n",
    "        <td>0.62</td>\n",
    "        <td>34865.07</td>\n",
    "      </tr>\n",
    "      <tr style=\"color: green\">\n",
    "        <td>Gradient Boosting</td>\n",
    "        <td>0.80</td>\n",
    "        <td>25176.16</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td><b>Decision Tree</b></td>\n",
    "        <td><b>0.63</b></td>\n",
    "        <td><b>34551.17</b></td>\n",
    "      </tr>\n",
    "    </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the Gradient Boosting regressor is the winner, we will now inspect its coeficients and interceptors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's predict an observation never seen before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do that we first build a function that takes a simple user input and transform it to a one hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_input = [2010, 124999.5, 6, 'Diesel', 'BMW']\n",
    "user_input = {'Year':2006, 'Mileage':82499.5, 'Make':'BMW'}\n",
    "def input_to_one_hot(data):\n",
    "    # initialize the target vector with zero values\n",
    "    enc_input = np.zeros(53)\n",
    "    # set the numerical input as they are\n",
    "    enc_input[0] = data['Year']\n",
    "    enc_input[1] = data['Mileage']\n",
    "#     enc_input[2] = data['Make']\n",
    "    ##################### Mark #########################\n",
    "    # get the array of marks categories\n",
    "    Makes = df.Make.unique()\n",
    "    # redefine the the user inout to match the column name\n",
    "    redefinded_user_input = 'Make_'+data['Make']\n",
    "    # search for the index in columns name list \n",
    "    make_column_index = X.columns.tolist().index(redefinded_user_input)\n",
    "    #print(mark_column_index)\n",
    "    # fullfill the found index with 1\n",
    "    enc_input[make_column_index] = 1\n",
    "    ##################### Fuel Type ####################\n",
    "    # get the array of fuel type\n",
    "#     fuel_types = df.fuel_type.unique()\n",
    "    # redefine the the user inout to match the column name\n",
    "#     redefinded_user_input = 'fuel_type_'+data['fuel_type']\n",
    "    # search for the index in columns name list \n",
    "#     fuelType_column_index = X.columns.tolist().index(redefinded_user_input)\n",
    "    # fullfill the found index with 1\n",
    "#     enc_input[fuelType_column_index] = 1\n",
    "    return enc_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(input_to_one_hot(user_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = input_to_one_hot(user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_pred = gbr.predict([a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_pred[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "\n",
    "joblib.dump(gbr, 'model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbr = joblib.load('model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"the best price for this BMW is\",gbr.predict([a])[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a REST API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://127.0.0.1:5000/api\"\n",
    "data = json.dumps({'Year':2014, 'Mileage':82499.5,  'Make':'BMW'})\n",
    "\n",
    "r = requests.post(url, data)\n",
    "\n",
    "print(r.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.json()['results'][0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
